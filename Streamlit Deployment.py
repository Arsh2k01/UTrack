#!/usr/bin/env python
# coding: utf-8

# ## Streamlit Web Deployment

# In[ ]:


get_ipython().system('pip install -q tf-models-official==2.3.0')
get_ipython().system('pip install streamlit')
get_ipython().system('pip install pyngrok')


# In[ ]:


from google.colab import drive
drive.mount('/content/drive')


# In[ ]:


get_ipython().run_cell_magic('writefile', 'utilss.py', 'import nltk\nnltk.download(\'punkt\')\nnltk.download(\'stopwords\')\nnltk.download(\'averaged_perceptron_tagger\')\nnltk.download(\'wordnet\')\nimport re\nimport string\nimport random\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport tweepy\nconsumerKey = "VEyxpXLGHG9USYhM7spHVKl36"\nconsumerSecret = "FG61nlBuLR7mb6UCPGxHH4UdMqwYNwL6aFhDt9gQJcaChblOkL"\naccessToken = "1142865475459846145-5VQ9CRY7iRlneurWdNzwHmT4Y9k6L1"\naccessTokenSecret = "iAkL3XWrsBdBQWn2eH8ifqnjoWkvBF5EHvJ1SsH6EcfLB"\n\nauthenticate = tweepy.OAuthHandler(consumerKey, consumerSecret) \nauthenticate.set_access_token(accessToken, accessTokenSecret) \napi = tweepy.API(authenticate, wait_on_rate_limit = True)\nfrom official.modeling import tf_utils\nfrom official import nlp\nfrom official.nlp import bert\n\n# Load the required submodules\nimport official.nlp.bert.bert_models\nimport official.nlp.bert.configs\nimport official.nlp.bert.tokenization as tokenization\nimport PIL\nimport pandas as pd\nimport numpy as np\nimport io\nimport tensorflow_hub as hub\n\nfrom keras.layers import Input, Dropout, Dense, Activation\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nmodule_url = \'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\'\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n#preprocessing\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport streamlit as st\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = ["[CLS]"] + text + ["[SEP]"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\nmodel_lonely = keras.models.load_model(\'/content/drive/MyDrive/Utrack_Models/Utrack_Lonely\')\n\ndef Show_Recent_Tweets(raw_text):\n  posts = api.user_timeline(screen_name=raw_text, count = 100, lang ="en", tweet_mode="extended")   \n  def get_tweets():\n    column_names = [\'tweet\', \'time\']\n    user = pd.DataFrame(columns =column_names)\n    \n    tweet_time = []\n    tweet_text = []\n    for info in posts[:100]:\n      tweet_time.append(info.created_at)\n      tweet_text.append(info.full_text)\n    \n    user[\'time\'] = tweet_time\n    user[\'tweet\'] = tweet_text\n    \n    return user\n \n  recent_tweets=get_tweets()        \n  return recent_tweets\n \ndef tokenize_tweets(clown) :\n    tweets = clown.tweet.tolist()\n    tokenizer = WordPunctTokenizer() \n    cleaned = []\n    for i in range(0, len(tweets)):\n        text = tweets[i]\n        text = re.sub(\'^https?://.*[rn]*\',\'\', text)\n        text = re.sub(r\'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\', \'\', text)\n        text = re.sub("(@[A-Za-z0-9_]+)","", text)\n        text = re.sub("([^\\w\\s])", "", text)\n        text = re.sub("^RT", "", text)\n        text = tokenizer.tokenize(text)\n        element = [text]\n        cleaned.append(element)\n    return cleaned\n\ndef lemmatize_sentence(tweet_tokens, stop_words = ()):\n    lemmatizer = WordNetLemmatizer()\n    cleaned_tokens = []\n    for token, tag in pos_tag(tweet_tokens):\n        if tag.startswith(\'NN\'):\n            pos = \'n\'\n        elif tag.startswith(\'V\'):\n            pos = \'v\'\n        else:\n            pos = \'a\'\n        token = lemmatizer.lemmatize(token, pos)\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\ndef create_lemmatized_sent(words):\n    cleaned = []\n    stop_words = stopwords.words(\'english\')\n    for i in range(0, len(words)):\n        sent = lemmatize_sentence(words[i][0], stop_words)\n        if len(sent) >= 0:\n            element = [sent]\n            cleaned.append(element)\n    return cleaned\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile("["\n                           u"\\U0001F600-\\U0001F64F"  # emoticons\n                           u"\\U0001F300-\\U0001F5FF"  # symbols & pictographs\n                           u"\\U0001F680-\\U0001F6FF"  # transport & map symbols\n                           u"\\U0001F1E0-\\U0001F1FF"  # flags (iOS)\n                           u"\\U00002702-\\U000027B0"\n                           u"\\U000024C2-\\U0001F251"\n                           "]+", flags=re.UNICODE)\n    return emoji_pattern.sub(r\'\', string)\n\ndef write_sent(clown, sent):\n    cleaned = []\n    for i in sent:\n        s = ""\n        for j in i[0]:\n            j = str(j)\n            j = j + " "\n            s = s + j\n        s = remove_emoji(s)\n        element = [s]\n        cleaned.append(element)\n    df = pd.DataFrame(cleaned,columns = [\'text\'])\n    # print(df.iloc[1])\n    #df.to_csv(\'cleaned_clown_1.csv\', index=False)\n    df1 = clown\n    df1 = df1[\'time\']\n    big = pd.concat([df, df1], axis = 1)\n    return big\n  \ndef import_and_predict(df, model):\n  # file = st.file_uploader(\'Please insert text\', type = ["csv"])\n  max_len = 150\n  # df = df.drop(columns = [\'Unnamed: 0\'])\n  \n  test_input = bert_encode(df["text"].values, tokenizer, max_len=max_len)\n  prediction = model.predict(test_input)\n\n  return prediction\n\n\ndef output_dataframe(df, prediction):\n  df2 = pd.concat([df, prediction], axis = 1)\n  return df2\n\ndef visualisation(n, file):\n  #constructing data\n  df = file\n  # df.drop([\'Unnamed: 0\',\'Unnamed: 0.1\',\'sigmoid_predictions\', \'normalised_predictions\'], axis=1, inplace=True)\n  jscolumn = df.predictions\n\n  df[\'final\'] = jscolumn\n  df[\'perc\'] = 100*(df.final)\n  new_df = df.drop(columns = [\'predictions\',\'final\'])\n  plt.figure(figsize=(40,15))\n  n = int(input())\n  temp_df = new_df[:n]\n  final_df  = temp_df.iloc[::-1]\n  sns.lineplot(x=\'time\', y=\'perc\', data=final_df, linewidth=7, color = \'red\')\n  plt.title("Mental State vs Date", fontsize= 40,fontweight=\'bold\')\n  # plt.xticks(rotation=90)\n  sns.set_style(\'white\')\n\n  plt.xlabel(\'Month\',fontsize=30,fontweight=\'bold\')\n  plt.xticks(fontsize=20,rotation=90)\n  plt.ylabel(\'Percentage\',fontsize=30,fontweight=\'bold\')\n  plt.yticks(fontsize=25)\n  plt.grid(axis=\'y\', alpha=0.5)\n\n  st.pyplot()\n\n  monthdict = {"January":1, "February":2, "March":3, "April":4, "May":5, "June":6, "July":7, "August":8,\n                      "September":9, "October":10, "November":11, "December":12}\n  values= []\n  for month in df.months.unique():\n    dftempo = df[pd.to_datetime(df[\'time\']).dt.month == monthdict[month]]\n    values.append(jsmean(dftempo.final))\n      \n  plt.figure(figsize=(15,10))\n  x= df.months.unique()\n  height = 100*np.array(values)\n  plt.bar(x, height, width=0.5, bottom=None, align=\'center\', color=[\'#78C850\',  # Grass\n                      \'#f20a53\',  # Fire\n                      \'#6890F0\',  # Water\n                      \'#A8B820\',  # Bug\n                      \'#A8A878\',  # Normal\n                      \'#A040A0\',  # Poison\n                      \'#F8D030\',  # Electric\n                      \'#E0C068\',  # Ground\n                      \'#EE99AC\',  # Fairy\n                      \'#C03028\',  # Fighting\n                      \'#6cf5d3\',                                                               \n                      \'#561191\'\n                    ])\n\n\n  sns.set_style(\'white\')\n\n\n  plt.xlabel(\'Month\',fontsize=15,fontweight=\'bold\')\n  plt.xticks(fontsize=15,rotation=45)\n  plt.ylabel(\'Percentage\',fontsize=15,fontweight=\'bold\')\n  plt.yticks(fontsize=15)\n  plt.grid(axis=\'y\', alpha=0.5)\n  plt.title(\'Average Percentage across months\', fontsize=20)\n\n  st.pyplot()\n\n  df[\'months\'] = df[\'time\'].dt.month_name()\n  plt.figure(figsize=(10,5))\n  sns.set_style(\'white\')\n  sns.swarmplot(x=\'months\', y=\'perc\', data=df.iloc[::-1])\n  #plt.xticks(rotation=90);\n  plt.xlabel(\'Month\',fontsize=15,fontweight=\'bold\')\n  plt.xticks(fontsize=15,rotation=0)\n  plt.ylabel(\'Percentage\',fontsize=15,fontweight=\'bold\')\n  plt.yticks(fontsize=15)\n  plt.title(\'Percentage across months\', fontsize=20)\n  plt.grid(axis=\'y\', alpha=0.5)\n\n  st.pyplot()\n\n  plt.figure(figsize=(10,6))\n \n  sns.violinplot(x=\'months\',\n                y=\'perc\', \n                data=df.iloc[::-1], \n                inner=None)\n  \n  sns.swarmplot(x=\'months\', \n                y=\'perc\', \n                data=df.iloc[::-1], \n                color=\'k\',\n                alpha=1) \n  plt.grid(axis=\'y\', alpha=0.5)\n  plt.xlabel(\'Month\',fontsize=15,fontweight=\'bold\')\n  plt.xticks(fontsize=15,rotation=0)\n  plt.ylabel(\'Percentage\',fontsize=15,fontweight=\'bold\')\n  plt.yticks(fontsize=15)\n  plt.title(\'Percentage across months\', fontsize=20)\n\n  st.pyplot()\n\n  def make_pie(sizes, text, colors):\n    import matplotlib.pyplot as plt\n    import numpy as np\n    sizes = [100-100*jsmean(df[\'final\']), 100*jsmean(df[\'final\'])]\n    text = round(jsmean(df[\'final\'])*100,2)\n    col = [[i/255. for i in c] for c in colors]\n\n    fig, ax = plt.subplots()\n    ax.axis(\'equal\')\n    width = 0.30\n    kwargs = dict(colors=col, startangle=90)\n    outside, _ = ax.pie(sizes, radius=1, pctdistance=1-width/2,**kwargs)\n    plt.setp( outside, width=width, edgecolor=\'white\')\n\n    kwargs = dict(size=20, fontweight=\'bold\', va=\'center\')\n    ax.text(0, 0, text, ha=\'center\', **kwargs)\n    plt.show()\n\n  c2 = (226,33,0)\n  c1 = (40,133,4)\n\n  make_pie([257,90], round(df[\'perc\'].mean(), 2),[c1,c2])\n  \n  st.pyplot()\n\ndef probability_out(x):\n  n=len(x)\n  for i in range(n):\n    if(x.iloc[i,0]<0): \n      x.iloc[i,0]=0\n    if(x.iloc[i,0]>=0 and x.iloc[i,0]<=1):\n      x.iloc[i,0]=np.sin(x.iloc[i,0])\n    if(x.iloc[i,0]>1):\n      x.iloc[i,0]=(np.log(x.iloc[i,0])+(np.pi)*(np.pi)*(np.sin(1)))/((np.pi)**2)\n    if(x.iloc[i,0]>1):\n      x.iloc[i,0]=1\n  return x\n\ndef tweets_conclusion(df):\n  #compute weights\n  def weight(x):\n    return (np.exp(x)-1)/(np.exp(1)-1)\n\n  def jsmean(arr):\n    num = 0\n    den = 0\n    for i in arr:\n        den = den + weight(i)\n        num = num + i*weight(i)\n    return (num/den)[0]\n\n  new_df = df.values\n  return jsmean(new_df)\n\ndef combine_all(user_name):\n  #preprocessing input data\n  raw_text = user_name\n  recent_tweets=Show_Recent_Tweets(raw_text)\n  words = tokenize_tweets(recent_tweets)\n  sent = create_lemmatized_sent(words)\n  df = write_sent(recent_tweets, sent)\n  #loading models\n  \n  us = f"Setting up models for analysing the profile of **{api.get_user(screen_name=raw_text).name}**"\n  st.markdown(us)\n\n  st.text("Loading the model")\n  model_lonely = keras.models.load_model(\'/content/drive/MyDrive/Utrack_Models/Utrack_Lonely\')\n  model_stress = keras.models.load_model(\'/content/drive/MyDrive/Utrack_Models/Utrack_Stress\')\n  model_anxiety = keras.models.load_model(\'/content/drive/MyDrive/Utrack_Models/Utrack_Anxiety\')\n  \n  intro = f"Twitter Bio of the user =>  **{api.get_user(screen_name=raw_text).description}**"\n  st.markdown(intro)\n\n  bio = f"User lives in **{api.get_user(screen_name=raw_text).location}**"\n  st.markdown(bio)\n\n  fol = f"Number of Followers of the user => **{api.get_user(screen_name=raw_text).followers_count}**"\n  st.markdown(fol)\n\n  st.text("Hold Up!! Working on Predictions...")\n\n  prediction_lonely = import_and_predict(df, model_lonely)\n  prediction_stress = import_and_predict(df, model_stress)\n  prediction_anxiety = import_and_predict(df, model_anxiety)\n\n  st.text("Predictions Done")\n  \n  col1, col2, col3 = st.beta_columns(3)\n\n  prediction_lonely = pd.DataFrame(prediction_lonely, columns = [\'Loneliness\'])\n  prediction_stress = pd.DataFrame(prediction_stress, columns = [\'Stress\'])\n  prediction_anxiety = pd.DataFrame(prediction_anxiety, columns = [\'Anxiety\'])\n  \n  prediction_lonely = probability_out(prediction_lonely)\n  prediction_stress = probability_out(prediction_stress)\n  prediction_anxiety = probability_out(prediction_anxiety)\n\n  #df_lonely = output_dataframe(df, prediction_lonely)\n  #df_stress = output_dataframe(df, prediction_stress)\n  #df_anxiety = output_dataframe(df, prediction_anxiety)\n  \n  df_total = output_dataframe(df,prediction_lonely)\n  df_total = output_dataframe(df_total,prediction_stress)\n  df_total = output_dataframe(df_total,prediction_anxiety)\n\n  st.write(df_total)\n  df_total = df_total.rename(columns={\'time\':\'index\'}).set_index(\'index\')\n  \n  with col1:\n    st.text("LONELINESS LEVELS")\n    st.success(tweets_conclusion(prediction_lonely))\n    st.line_chart(data=df_total[\'Loneliness\'])\n\n  with col2:\n    st.text("STRESS LEVELS")\n    st.success(tweets_conclusion(prediction_stress))\n    st.line_chart(data=df_total[\'Stress\'])\n  \n  with col3:\n    st.text("ANXIETY LEVELS")\n    st.success(tweets_conclusion(prediction_anxiety))\n    st.line_chart(data=df_total[\'Anxiety\'])\n  \n  #return prediction_lonely\n  #streamlit.line_chart(data=None, width=0, height=0, use_container_width=True)\n   \n  #visualisation(n=50, df_lonely)\n  #visualisation(n=50, df_stress)\n  #visualisation(n=50, df_anxiety)')


# In[ ]:


get_ipython().run_cell_magic('writefile', 'app.py', 'from utilss import combine_all\nimport tensorflow as tf\nimport streamlit as st\nfrom tensorflow import keras\nst.set_option(\'deprecation.showfileUploaderEncoding\', False)\n#@st.cache(allow_output_mutation=True)\nst.set_page_config(\n     page_title="UTrack",\n     layout="wide"\n)\nst.title("UTrack")\n#st.write("#")\n\nst.subheader(\'*Analysing Twitter Users on Tweet-to-Tweet basis to track levels of Loneliness, Stress & Anxiety*\')\n\nraw_text = st.text_input("Enter the exact twitter handle of the Personality (without @)")\nst.text(raw_text)\nif raw_text == \'\':\n  st.text(\'OOPS!!!! Enter userID\')\nelse:\n  combine_all(raw_text)\n\n  # pred_lonely = import_and_predict(file, model_lonely)\n  # pred_stress = import_and_predict(file, model_stress)\n  # pred_anxious = import_and_predict(file, model_anxious)\n  # string = "Done"\n  # st.text(pred_lonely)\n  # st.text(pred_stress)\n  # st.text(pred_anxious)\n  # st.success(string)\n\n  ')


# ## Running  localhost server for colab from ngrok

# In[ ]:


get_ipython().system('ngrok authtoken 1pqPDOU30ORUzHtrlCA5DX7odxX_4N3in7gRue2ctUDTBYPun')


# In[ ]:


get_ipython().system('nohup streamlit run app.py --server.port 80 &')


# In[ ]:


from pyngrok import ngrok

url = ngrok.connect(port=80)
url


# In[ ]:


get_ipython().system('cat /content/nohup.out')


# In[ ]:


# ! killall ngrok


# In[ ]:




